# -*- coding: utf-8 -*-
"""TitanicDataSet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LWclWXC49EjAeSlUVpetQj3F-9Xp9Vwd
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sn

from sklearn.tree import DecisionTreeClassifier

from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn import metrics
from sklearn.metrics import accuracy_score, confusion_matrix,roc_curve, roc_auc_score

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA
from sklearn.ensemble import ExtraTreesClassifier    # Feature importance selection

from imblearn.over_sampling import RandomOverSampler

titanicData = pd.read_csv("https://raw.githubusercontent.com/BigDataGal/Python-for-Data-Science/master/titanic-train.csv")

titanicData

#!pip install pandas_profiling==2.8.0

'''
import pandas_profiling
titanicReport = pandas_profiling.ProfileReport(titanicData)
titanicReport.to_file("titanicReport.html")
'''

"""# EDA:

Columns: Tiket and Cabit have "High Cardinality", and Cabin have lot (77.7%) of missing values.

Column: Name have the unique values

Column: Age have NAN values, so replaced with mean value

Column: Sex, as its a Object type, so used 1-hot encoding as there is no order required along with dummy variable trap

Column: PClass has 3 classes, 1 - upper, 2 - Moderate, & 3 - lower class. The ship was first hit at the 3rd class area, so the chances of surival is less.
"""

titanicData.isnull().sum()

x = titanicData[["Pclass","Sex","Age","SibSp","Parch","Fare"]]

y = titanicData["Survived"]

x.isnull().sum()

x.shape,y.shape

x.info()

x.describe()

x.Sex = pd.get_dummies(x.Sex,drop_first=True)

x.head()

x.isnull().sum()

"""Replacing the NAN values in AGE column W.R.T the PClass mean value of indiviual class."""

classAvg = []
for i in range(1,4):
  classAvg.append(int((x.Age[x.Pclass==i]).mean()))

classAvg

def impute_age(cols):
  Age = cols[0]
  Pclass = cols[1]

  if pd.isnull(Age):

    if Pclass == 1:
      return classAvg[0]
    elif Pclass == 2:
      return classAvg[1]
    elif Pclass == 3:
      return classAvg[2]

  else:
    return Age

x["Age"] = x[["Age","Pclass"]].apply(impute_age,axis=1)

x.isnull().sum()

"""Feature Importance with Extra Trees Classifier"""

model = ExtraTreesClassifier()
model.fit(x,y)
print(model.feature_importances_)

x_impo_features = x[["Pclass","Sex","Age","Fare"]]

# Balancing the predicted entries using RandomOverSampling

ros = RandomOverSampler(random_state=0)
Xnew,Ynew = ros.fit_sample(x_impo_features,y)

Xnew.shape,Ynew.shape

x_train,x_test,y_train,y_test = train_test_split(Xnew,Ynew,test_size=0.30,random_state=44)

clf = DecisionTreeClassifier()

clf.fit(x_train,y_train)

clf.score(x_test,y_test)

scalar = StandardScaler()

x_transform = scalar.fit_transform(Xnew)

x_train,x_test,y_train,y_test = train_test_split(x_transform,Ynew,test_size=0.30,random_state=44)

clf.fit(x_train,y_train)

clf.score(x_test,y_test)

grid_param = {
    'criterion': ['gini', 'entropy'],
    'max_depth' : range(2,32,1),
    'min_samples_leaf' : range(1,10,1),
    'min_samples_split': range(2,10,1),
    'splitter' : ['best', 'random']
    
}

grid_search = GridSearchCV(estimator=clf,
                     param_grid=grid_param,
                     cv=5,
                    n_jobs =-1)

grid_search.fit(x_train,y_train)

grid_search.best_params_

grid_search.best_score_

clf = DecisionTreeClassifier(criterion='entropy',max_depth=26,min_samples_leaf=1,min_samples_split=2,splitter='random')

clf.fit(x_train,y_train)

clf.score(x_test,y_test)

y_pred = clf.predict(x_test)

accuracy_score(y_test,y_pred)

confusionMatrix = confusion_matrix(y_test,y_pred)

print(metrics.f1_score(y_test,y_pred))

print(metrics.classification_report(y_test,y_pred))

